{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS6713\n",
    "## Homework 2\n",
    "\n",
    "Write your name/abc123 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Ben Hoffman\n",
    "\n",
    "**abc123**: peo042\n",
    "\n",
    "Remember, it is okay to talk to your classmates about the homework, but do not share solutions. You should complete the homework on your own. Below, please list all of your peers (\"collaborators\") that you discussed the homework with.\n",
    "\n",
    "**Collaborators**:\n",
    "- Name 1\n",
    "- Name 2\n",
    "- Name 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2\n",
    "\n",
    "### Introduction\n",
    "You are a data scientist working for the government. You want to understand the public opinion regarding hurricane Maria which is responsible for killing at least 499 people in Puerto Rico. Total losses are estimated at $94.4 billion dollars which accrued to government agencies, businesses, and more importantly, familes [1]. With this background, whether you are a politician, bussiness person, or one effected by the hurricane, understanding the sentiment of the general populace is important. For this assigment, you will use a subset of the tweets retrieved from Twitter that mentioned #PuertoRico over the period of October 4 to November 7, 2017 [2] to measure the sentiment (i.e., the \"good\" or \"bad\" opinions people have about the hurricane and its impact) of this event.\n",
    "\n",
    "For this task, we will write code for a lexicon-based analysis (i.e., lexicon-based classification). Lexicon-based classification is a way to categorize text based using manually generated lists of topical words. Essentially, we just need to check if the topical words appear in a piece of text (e.g., a tweet). In this exercise we will make use of manually curated sentiment words. However, the basic experimental process is the same for other tasks (e.g., identifying offensive language).\n",
    "\n",
    "If you are interested, though it is not needed, you can learn more about lexicon-based classification in Chapter 21 (21.2 and 21.6) of the free online book at the following link: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/21.pdf)\n",
    "\n",
    "### References\n",
    "[1]  Spalding, Rebecca (November 13, 2017). \"Puerto Rico Seeks $94 Billion in Federal Aid for Hurricane Recovery\". Bloomberg News. Retrieved December 15, 2017.\n",
    "\n",
    "[2]  site: https://archive.org/details/puertorico-tweets\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "After completing the exercises below, generate a pdf of the code **with** outputs. After that create a zip file containing both the completed exercise and the generated PDF/HTML. You are **required** to check the PDF/HTML to make sure all the code **and** outputs are clearly visible and easy to read. If your code goes off the page, you should reduce the line size. I generally recommend not going over 80 characters.\n",
    "\n",
    "For this task, unzip and move the file \"puerto-rico.jsonl\" in to the same directory as this notebook, then complete the following exercises. However, when you turn the assigment in, do **NOT** include puerto-rico.jsonl in your zip file when you submit the homework, you will kill Blackboard.\n",
    "\n",
    "Finally, name the zip file using a combination of your the assigment and your name, e.g., ps2_zanella.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (10 points)\n",
    "\n",
    "The files \"positive_words.txt\" and \"negative_words.txt\" contain manually curated positive (e.g., good, great, awesome) and negative words (e.g., bad, hate, terrible). The files contain one word on each line. Write a function that takes the open file and adds the words (i.e., each line) to a set then returns it.\n",
    "\n",
    "**Note:** You should use \".strip()\" to remove the newline character from the end of each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "def file_to_set(file):\n",
    "    # Write code here\n",
    "    word = []\n",
    "    for line in file:\n",
    "        word.append(line.strip())\n",
    "    words = set(word)\n",
    "    return words # You should return a set\n",
    "\n",
    "positive_file = open('./positive-words.txt', encoding='utf8')\n",
    "positive_words = file_to_set(positive_file)\n",
    "positive_file.close()\n",
    "\n",
    "negative_file = open('./negative-words.txt', encoding='iso-8859-1') # If you get a weird read error. Let me know. We can change the encoding.\n",
    "negative_words = file_to_set(negative_file)\n",
    "negative_file.close()\n",
    "\n",
    "poswords = file_to_set(positive_words)\n",
    "negwords = file_to_set(negative_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(positive_words) == type(set(poswords)))\n",
    "assert(type(negative_words) == type(set(negwords)))\n",
    "assert(len(positive_words) == 2006)\n",
    "assert(len(negative_words) == 4783)\n",
    "assert(('good' in positive_words)  == True)\n",
    "assert(('bad' in negative_words)  == True)\n",
    "assert(('bad' not in positive_words) == True)\n",
    "print(\"Asserts finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (10 points)\n",
    "\n",
    "For this exercise, you need to write a function that counts the number of words in a sentence that also appear in a set. For example, given the set set(['good', 'great']) and the sentence \"this is good good good\", the function should return 3.\n",
    "\n",
    "**Hint:** You can check if something is in a set using the following notation:\n",
    "\n",
    "```python\n",
    "mySet = set([\"a\", \"b\", \"c\"])\n",
    "otherList = [\"c\", \"d\"]\n",
    "for letter in otherList:\n",
    "    if letter in mySet:\n",
    "        print(letter)\n",
    "```\n",
    "\n",
    "The above code will print \"c\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentiment_words(sentiment_set, tweet_text, lower):\n",
    "    # Your code here\n",
    "    word = []\n",
    "    cnt = 0\n",
    "    for line in sentiment_set:\n",
    "        word.append(line.strip())\n",
    "    words = set(word)\n",
    "    \n",
    "    if lower == True:\n",
    "        tweet = tweet_text.lower().replace('\\t',' ').split(\" \")\n",
    "    else:\n",
    "        tweet = tweet_text.replace('\\t',' ').split(\" \")\n",
    "    \n",
    "    for w in tweet:\n",
    "        if w in words:\n",
    "            cnt +=1\n",
    "    return cnt #You should return a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(count_sentiment_words(positive_words, \"this is a good good good class\", True) == 3)\n",
    "assert(count_sentiment_words(positive_words, \"this is a good\\tgood\\tgood class\", True) == 3)\n",
    "assert(count_sentiment_words(positive_words, \"this is a GOOD GOOD GOOD class\", False) == 0)\n",
    "assert(count_sentiment_words(positive_words, \"Python is the best programming language for data science\", True) == 1)\n",
    "assert(count_sentiment_words(negative_words, \"R is bad compared to Python ;)\", True) == 1)\n",
    "print(\"Asserts finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (10 point)\n",
    "\n",
    "For this exercise, you will write a function that takes two numbers as input and returns a string. Intuitively, this is a basic classification function for lexicon-based sentiment classification. \n",
    "\n",
    "The function should take as input parameters the the number of positive (num_pos_words) and negative (num_neg_words) words in each tweet to predict sentiment. If the number of positive words is greater than to the number of negative tweets (num_pos_words > num_neg_words), then predict **\"positive\"**. If the number of negative words is greater than the number of positive words (num_neg_words > num_pos_words), then predict **\"negative\"**. If both num_pos_words and num_neg_words are equal (num_neg_words = num_pos_words), predict **\"neutral\"**. This is known as lexicon-based classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(num_pos_words, num_neg_words):\n",
    "    # Your code here\n",
    "    if num_pos_words > num_neg_words:\n",
    "        output = 'positive'\n",
    "    elif num_pos_words < num_neg_words:\n",
    "        output = 'negative'\n",
    "    else:\n",
    "        output = 'neutral'\n",
    "    return output # You should return a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(predict(2, 5) == 'negative')\n",
    "assert(predict(5, 2) == 'positive')\n",
    "assert(predict(3, 3) == 'neutral')\n",
    "print(\"Assert finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (10 point)\n",
    "\n",
    "This exercise is similar to Exercise 3. However, instead of making a prediction, we should write a function that returns a sentiment score. Specifically, assume num_pos_words is 3 and num_neg_words is 4, the function should return -1. The idea is that the more *positive* the number, the more positive the sentiment. Likewise, the more *negative* the number, the more negative the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(num_pos_words, num_neg_words):\n",
    "    # Your code here\n",
    "    score = num_pos_words - num_neg_words\n",
    "    return score # You should return a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(predict_score(3, 1) == 2)\n",
    "assert(predict_score(2, 2) == 0)\n",
    "assert(predict_score(2, 5) == -3)\n",
    "print(\"Asserts finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (10 point)\n",
    "\n",
    "Write a function that takes a json string as input and returns a Python object. Hint: This can be one line. You can use the json library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json_string_to_dictionary(json_string):\n",
    "    # Your code here\n",
    "    string = json.loads(json_string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert finished successfully!\n"
     ]
    }
   ],
   "source": [
    "data = json_string_to_dictionary('{\"a\": 1}')\n",
    "assert(data == {'a': 1})\n",
    "data = json_string_to_dictionary('[1,2,3]')\n",
    "assert(data == [1,2,3])\n",
    "print(\"Assert finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (30 points)\n",
    "\n",
    "For this task, we combine the functions written for the previous exercises to classify all of the tweets in a real Twitter dataset. You should write code that does the following:\n",
    "1. Keeps track of the number of tweets\n",
    "2. Keeps track of the number of positive and negative tweets\n",
    "3. Keeps track of the user that tweets the most\n",
    "4. Keeps track of the total number of unique users\n",
    "5. Keeps track of the average number of tweets per user (how many tweets does each user make, on average)\n",
    "6. Keeps track of the most positive and negative tweets.\n",
    "\n",
    "Note: This task depends on Exercises 1 through 5. You will need to complete them first. Also, do **not** store all of the tweets in a list.  This will use too much memory because of the size of the dataset. It is okay to store all of the user's screen names.\n",
    "\n",
    "Finally, the dataset is big! So, I recommend working on a subset of the dataset to make sure your code works, i.e., you could \"break\" after the first 100 lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_of_tweets = 0\n",
    "total_number_of_positive_tweets = 0\n",
    "total_number_of_negative_tweets = 0\n",
    "most_positive_tweet = 0\n",
    "most_negative_tweet = 0\n",
    "user_with_most_tweets = 0\n",
    "average_number_tweets_per_user = 0\n",
    "total_number_of_users = 0\n",
    "\n",
    "users_freq = {}\n",
    "users_tot = list()\n",
    "most_tweets = 0\n",
    "min_sent = 0\n",
    "max_sent = 0\n",
    "    \n",
    "# NOTE: You may need to define extra variables to help generate the required output.\n",
    "\n",
    "twitter_dataset = open('puerto-rico_tweets.jsonl', 'r')\n",
    "\n",
    "for row in twitter_dataset:\n",
    "    tweet_dict = json_string_to_dictionary(row)\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    tweet_text = tweet_dict['full_text'] # MODIFY THIS LINE TO GET THE \"full_text\" from the tweet_dict\n",
    "    screen_name = tweet_dict['user']['screen_name']  # MODIFY THIS LINE TO GET THE \"screen_name\" from the tweet_dict\n",
    "    \n",
    "    users_tot.append(screen_name)\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    num_pos_words = count_sentiment_words(positive_words, tweet_text, True)\n",
    "    num_neg_words = count_sentiment_words(negative_words, tweet_text, True)\n",
    "    \n",
    "    sentiment_prediction = predict(num_pos_words, num_neg_words)\n",
    "    sentiment_score = predict_score(num_pos_words, num_neg_words)\n",
    "    \n",
    "    ################################\n",
    "    # Your code should do the following:\n",
    "    #   1. Keep track of the number of tweets\n",
    "    #   2. Keep track of the number of positive and negative tweets\n",
    "    #   3. Keep track of the user that tweet the most\n",
    "    #   4. Keep track of the total number of unique users\n",
    "    #   5. Keep track of the average number of tweets per user (how many tweets does each user make, on average)\n",
    "    #   6. Keep track of the most positive and negative tweets.\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    total_number_of_tweets += 1\n",
    "    \n",
    "    if sentiment_score > 0:\n",
    "        total_number_of_positive_tweets += 1\n",
    "        if max_sent == 0:\n",
    "            max_sent = sentiment_score\n",
    "            most_positive_tweet = tweet_text\n",
    "        elif sentiment_score > max_sent:\n",
    "            max_sent = sentiment_score\n",
    "            most_positive_tweet = tweet_text\n",
    "\n",
    "    elif sentiment_score < 0:\n",
    "        total_number_of_negative_tweets += 1\n",
    "        if min_sent == 0:\n",
    "            min_sent = sentiment_score\n",
    "            most_negative_tweet = tweet_text\n",
    "        elif sentiment_score < min_sent:\n",
    "            min_sent = sentiment_score\n",
    "            most_negative_tweet = tweet_text\n",
    "\n",
    "    ################################\n",
    "\n",
    "# You may need to have code after the for loop, depending on your implementation\n",
    "# CODE HERE (Maybe, depending on your implementation)\n",
    "twitter_dataset.close()\n",
    "\n",
    "total_number_of_users = len(set(users_tot))\n",
    "\n",
    "average_number_tweets_per_user = total_number_of_tweets/total_number_of_users\n",
    "\n",
    "for i in users_tot:\n",
    "    if i not in users_freq:\n",
    "        users_freq[i] = 1\n",
    "    else:\n",
    "        users_freq[i] += 1\n",
    "\n",
    "for x in users_freq:\n",
    "    if users_freq[x] > most_tweets:\n",
    "        most_tweets = users_freq[x]\n",
    "        user_with_most_tweets = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Tweets: 4939\n",
      "Total Number of Positive Tweets: 1222\n",
      "Total Number of Negative Tweets: 813\n",
      "\n",
      "Most Positive Tweet\n",
      "RT @novaren: AND... THIS IS OUR GRAND TOTAL FOR #PUERTORICO! Thank you to everyone who donated time or money or a signal boost or support f…\n",
      "Sentiment score: 4\n",
      "\n",
      "Most Negative Tweet\n",
      "RT @bill_auclair: This is how desperate the people of #PuertoRico are to avoid dying from the sluggish response of an inept kakistocracy. #…\n",
      "Sentiment score: -4\n",
      "\n",
      "Total Number of Users: 4616\n",
      "Average Number of Tweets per User: 1.0699740034662044\n",
      "User with the most tweets: Noti_PuertoRico\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of Tweets: {}\".format(total_number_of_tweets))\n",
    "print(\"Total Number of Positive Tweets: {}\".format(total_number_of_positive_tweets))\n",
    "print(\"Total Number of Negative Tweets: {}\\n\".format(total_number_of_negative_tweets))\n",
    "\n",
    "print(\"Most Positive Tweet\")\n",
    "print(most_positive_tweet)\n",
    "print(\"Sentiment score: {}\".format(max_sent))\n",
    "print()\n",
    "\n",
    "print(\"Most Negative Tweet\")\n",
    "print(most_negative_tweet)\n",
    "print(\"Sentiment score: {}\".format(min_sent))\n",
    "print()\n",
    "\n",
    "print(\"Total Number of Users: {}\".format(total_number_of_users))\n",
    "print(\"Average Number of Tweets per User: {}\".format(average_number_tweets_per_user))\n",
    "print(\"User with the most tweets: {}\".format(user_with_most_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(isinstance(total_number_of_tweets, int) or isinstance(total_number_of_tweets, float))\n",
    "assert(isinstance(total_number_of_positive_tweets, int) or isinstance(total_number_of_positive_tweets, float))\n",
    "assert(isinstance(total_number_of_negative_tweets, int) or isinstance(total_number_of_negative_tweets, float))\n",
    "assert(isinstance(most_positive_tweet, str))\n",
    "assert(isinstance(most_negative_tweet, str))\n",
    "assert(isinstance(user_with_most_tweets, str))\n",
    "assert(total_number_of_tweets == 4939)\n",
    "print(\"Assert finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 (20 points)\n",
    "\n",
    "For this exercise, you will perform manual analysis of the predictions. Modify the code to load the tweet text, then answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1: RT @SamaritansPurse: You can help the many across #PuertoRico who remain in desperate need after #HurricaneMaria. See how here: https://t.c…\n",
      "Tweet 1 Prediction: negative\n",
      "\n",
      "Tweet 2: RT @daddy_yankee: La madre naturaleza está azotando con potencia a sus hijos. Mi corazón y mis oraciones con mi tierra #PuertoRico y mis he…\n",
      "Tweet 2 Prediction: neutral\n",
      "\n",
      "Tweet 3: RT @SteveCase: PLEASE HELP: @ChefJoseAndres is working tirelessly to feed #PuertoRico, but urgently needs our help: https://t.co/xrJRVC9Ks4…\n",
      "Tweet 3 Prediction: neutral\n",
      "\n",
      "Tweet 4: RT @StarrMSS: .@elvisduran gave 30K to @Bethenny to charter  plane to bring supplies to #PuertoRico #HurricaneMaria. He also gave 100K to @…\n",
      "Tweet 4 Prediction: neutral\n",
      "\n",
      "Tweet 5: RT @theCEI: THANK YOU to @rubendiazjr and the NY Hispanic Clergy for organizing an amazing event last week in support of #PuertoRico! 🇵🇷❤️…\n",
      "Tweet 5 Prediction: positive\n",
      "\n",
      "Tweet 6: RT @nasw: Staff @NASW_PuertoRico want social workers, news outlets, Congress to know about humanitarian crisis on island https://t.co/CROfH…\n",
      "Tweet 6 Prediction: negative\n",
      "\n",
      "Tweet 7: RT @santamonicamoe: Look at Bethenny Frankel's Twitter feed &amp; how she's helping #PuertoRico. It's mind boggling @Bethenny @maddow @MSNBC\n",
      "ht…\n",
      "Tweet 7 Prediction: positive\n",
      "\n",
      "Tweet 8: RT @ReutersUK: How solar energy saved a Puerto Rican farm from Hurricane Maria https://t.co/PFToTHE14u https://t.co/nrUoh43KwZ\n",
      "Tweet 8 Prediction: neutral\n",
      "\n",
      "Tweet 9: RT @MayorLevine: \"#PuertoRico is part of the USA. Let's treat them that way.\" #GetItDone 🇵🇷 @CarmenYulinCruz @JackieNBC6 @nbc6\n",
      "📺: https://t…\n",
      "Tweet 9 Prediction: neutral\n",
      "\n",
      "Tweet 10: Helping the #disability community in #PuertoRico https://t.co/hbbZIqpApX\n",
      "Tweet 10 Prediction: positive\n",
      "\n",
      "Tweet 11: RT @debbiesideris: When @KellyannePolls repays OUR taxpayer $ I want mine to go to #USVI &amp; #PuertoRico \n",
      "#grifters \n",
      "#FireKellyAnne https://t…\n",
      "Tweet 11 Prediction: neutral\n",
      "\n",
      "Tweet 12: @SenWarren Re: PuertoRico-Drone tech shd b used to &amp; shd already have been used  to find ppl &amp; render aid. Was their lack of use deliberate?\n",
      "Tweet 12 Prediction: negative\n",
      "\n",
      "Tweet 13: RT @SecretarySonny: Important @NYTimes correction: @USDANutrition granted hot foods waiver to #PuertoRico on 9/30. It was NOT denied. https…\n",
      "Tweet 13 Prediction: positive\n",
      "\n",
      "Tweet 14: RT @operationbless: An update on our efforts to help #HurricaneMaria victims in #PuertoRico\n",
      "\n",
      "https://t.co/doUmRssGOP\n",
      "Tweet 14 Prediction: neutral\n",
      "\n",
      "Tweet 15: RT @GovChrisSununu: Everyone's up bright and early at the Statehouse for the 2nd day of the #PuertoRico aid drive. Make sure to stop by tod…\n",
      "Tweet 15 Prediction: positive\n",
      "\n",
      "Tweet 16: RT @TheArtNewspaper: Puerto Rico’s museums on the mend https://t.co/vK9GDc9BFN #PuertoRico #HurricaneMaria #Maria https://t.co/S58jWEvpkz\n",
      "Tweet 16 Prediction: neutral\n",
      "\n",
      "Tweet 17: RT @Reuters: U.S. House committee examining barriers to Puerto Rico recovery: official https://t.co/k3OkfJXxF9 https://t.co/eNBVRkdZhH\n",
      "Tweet 17 Prediction: neutral\n",
      "\n",
      "Tweet 18: RT @JanaKTVU: #PuertoRico - @CALNurses from San Francisco Bay Area arrived in San Juan today as part of 2wk @AFLCIO 300-person relief deleg…\n",
      "Tweet 18 Prediction: positive\n",
      "\n",
      "Tweet 19: So showing up &amp; throwing paper towels at them isn't enough?! \n",
      "\n",
      "#PuertoRico https://t.co/A196TgahQu\n",
      "Tweet 19 Prediction: neutral\n",
      "\n",
      "Tweet 20: RT @santamonicamoe: Look at Bethenny Frankel's Twitter feed &amp; how she's helping #PuertoRico. It's mind boggling @Bethenny @maddow @MSNBC\n",
      "ht…\n",
      "Tweet 20 Prediction: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "twitter_dataset = open('puerto-rico_tweets.jsonl', 'r')\n",
    "\n",
    "num_tweets_to_print = 20\n",
    "\n",
    "num_tweets = 0\n",
    "\n",
    "for row in twitter_dataset:\n",
    "    num_tweets += 1\n",
    "    tweet_dict = json_string_to_dictionary(row)\n",
    "    \n",
    "    ###############################\n",
    "    # YOUR CODE HERE\n",
    "    tweet_text = tweet_dict['full_text'] # MODIFY THIS LINE TO GET THE \"full_text\" from the tweet_dict    \n",
    "    ###############################\n",
    "    \n",
    "    num_pos_words = count_sentiment_words(positive_words, tweet_text, True)\n",
    "    num_neg_words = count_sentiment_words(negative_words, tweet_text, True)\n",
    "    \n",
    "    sentiment_prediction = predict(num_pos_words, num_neg_words)\n",
    "    \n",
    "    print(\"Tweet {}: {}\".format(num_tweets, tweet_text))\n",
    "    print(\"Tweet {} Prediction: {}\".format(num_tweets, sentiment_prediction))\n",
    "    print()\n",
    "    \n",
    "    if num_tweets == num_tweets_to_print:\n",
    "        break\n",
    "    \n",
    "twitter_dataset.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following tasks:\n",
    " \n",
    "- Manually annotate all of the tweets printed above:\n",
    "   1. Positive\n",
    "   1. Neutral\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "   1. Positive\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "   1. Positive\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "   1. Neutral\n",
    "   1. Negative\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "   1. Positive\n",
    "   1. Neutral\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "   1. Negative\n",
    "   1. Neutral\n",
    "\n",
    "- How many of the predictions are right or wrong compared to your annotations?\n",
    "    - I disagreed with 8 of the predictions\n",
    "    \n",
    "- Do you see any major limitations of lexicon-based classificaiton (i.e., making sentiment predictions using individual words)? Use your intuition, I will accept most answers, as long as it makes some sense. Please describe and provide examples below:\n",
    "\n",
    "The biggest limitation is sarcasm, like in number 19. The tweet itself is negative, but the sentiment is to point out the absurity of the situation. Also, many of the tweets were just information, which does not convey any sentiment. But with using a lexicon, these tweets could be classified as positive or negative depending on the wording, but are actually neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit (10 points)\n",
    "\n",
    "For this exercise, you should use a different dataset (email me if you want me to share a dataset with your directly, or you can use your own---see below for dataset resources) and analyze it with a different (non-sentiment) lexicon. You can complete the same analysis as above, or do something different as long as you make use of a new lexicon and a new dataset.\n",
    "\n",
    "Possible lexicons:\n",
    "- Hate speech and offensive language lexicons:\n",
    "    - https://github.com/steve050798/hate-speech-and-offensive-language/raw/master/lexicons/hatebase_dict.csv\n",
    "    - https://www.cs.cmu.edu/~biglou/resources/bad-words.txt\n",
    "- Mental (Health) Lexicons    \n",
    "    - Anxiety Lexicon\n",
    "        - https://github.com/lrheault/anxiety\n",
    "    - Depression Lexicon\n",
    "        - https://github.com/halolimat/Social-media-Depression-Detector\n",
    "    - Abuse Lexicon\n",
    "        - https://github.com/uds-lsv/lexicon-of-abusive-words\n",
    "        \n",
    "You may use one of the lexicons above or another non-sentiment lexicon of your choice. I may allow another sentiment lexicon if the new analysis is interesting; however, email me before doing this. An example would be using a Spanish sentiment lexicon to analyze Spanish text, or doing a different analysis than the one abouve (e.g., anayzing sentiment over time).\n",
    "\n",
    "Where to look for new data?\n",
    "- https://www.kaggle.com\n",
    "- https://www.docnow.io/catalog/ (Some datasets contain the text, not just tweet ids). If you have something specific you want, I can grab it for you using my developer account.\n",
    "- Google :). You do not need to use tweets.\n",
    "\n",
    "You can pull new data from Twitter, but you will need to create a developer account. The easiest way to pull new data is via the use of Twarc (https://github.com/DocNow/twarc).\n",
    "\n",
    "Finally, when you submit, I don't need to have your data, just make sure to provide a couple of examples, a link to the data (if available), and make sure all of the output of your code is printed with the output so I can analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and document your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
