{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "\n",
    "For this problem set, you will expand on PS2 to perform and evaluate various sentiment classification methods.\n",
    "\n",
    "Your name: Ben Hoffman\n",
    "\n",
    "You abc123: Peo042\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "After completing the exercises below, generate a pdf of the code **with** outputs. After that create a zip file containing both the completed exercise and the generated PDF/HTML. You are **required** to check the PDF/HTML to make sure all the code **and** outputs are clearly visible and easy to read. If your code goes off the page, you should reduce the line size. I generally recommend not going over 80 characters.\n",
    "\n",
    "Finally, name the zip file using a combination of your the assigment and your name, e.g., ps3_rios.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (10 point)\n",
    "\n",
    "For this step, you will load the **training dataset \"twitdata_TEST.tsv\"** and **test sentiment dataset \"allTrainingData.tsv\"**. The data should be loaded into 4 lists of strings: X_txt_train, X_txt_test, y_test, y_train.\n",
    "\n",
    "- Note 1: when using csvreader, you need to pass the \"quoting\" the value csv.QUOTE_NONE.\n",
    "- Note 2: the tweet is the 4th column (index=3) and the gold label (sentiment) is the 3rd column (index=2)\n",
    "- Note 3: a tsv file is a tab-separated csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#open each file into a useable object\n",
    "filetrain = list(csv.reader(open('allTrainingData.tsv'), delimiter ='\\t', quoting = csv.QUOTE_NONE))\n",
    "filetest = list(csv.reader(open('twitdata_TEST.tsv'), delimiter ='\\t', quoting = csv.QUOTE_NONE))\n",
    "\n",
    "#make empty lists for each variable\n",
    "X_txt_train = []\n",
    "y_train = []\n",
    "X_txt_test = []\n",
    "y_test = []\n",
    "\n",
    "#loop over each file to isolate the needed information into the empty lists\n",
    "for x in filetrain:\n",
    "    X_txt_train.append(x[3])\n",
    "    y_train.append(x[2])\n",
    "    \n",
    "for y in filetest:\n",
    "    X_txt_test.append(y[3])\n",
    "    y_test.append(y[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(X_txt_train) == type(list()))\n",
    "assert(type(X_txt_train[0]) == type(str()))\n",
    "assert(type(X_txt_test) == type(list()))\n",
    "assert(type(X_txt_test[0]) == type(str()))\n",
    "assert(type(y_test) == type(list()))\n",
    "assert(type(y_train) == type(list()))\n",
    "assert(len(X_txt_test) == 3199)\n",
    "assert(len(y_test) == 3199)\n",
    "assert(len(X_txt_train) == 8018)\n",
    "assert(len(y_train) == 8018)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (10 point)\n",
    "\n",
    "This part is similar to HW2 (using the positive_words and negative_words variables). We will compare last homework's lexicon-based classification method with supervised models. Only make predictions on the test split and store all predictions in the list lex_test_preds. Next, calculate the **micro** precision, recall, and f1 scores using the lex_test_preds list.\n",
    "\n",
    "- Note 1: use precision_score, recall_score, f1_score from sklearn.metrics with **average=\"micro\"** to calculate the micro precision, ....\n",
    "\n",
    "You can learn more about lexicon-based classification in Chapter 19.6. If you are interested, the chapter is available online for free at the following link: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/19.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THE CODE IN THIS CELL\n",
    "class LexiconClassifier():\n",
    "    def __init__(self):\n",
    "        self.positive_words = set()\n",
    "        with open('positive-words.txt', encoding = 'utf-8') as iFile:\n",
    "            for row in iFile:\n",
    "                self.positive_words.add(row.strip())\n",
    "\n",
    "        self.negative_words = set()\n",
    "        with open('negative-words.txt', encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.negative_words.add(row.strip())\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        num_pos_words = 0\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "            elif word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        \n",
    "        pred = 'neutral'        \n",
    "        if num_pos_words > num_neg_words:\n",
    "            pred = 'positive'\n",
    "        elif num_pos_words < num_neg_words:\n",
    "            pred = 'negative'\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def count_pos_words(self, sentence):\n",
    "        num_pos_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "        return num_pos_words\n",
    "\n",
    "    def count_neg_words(self, sentence):\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        return num_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5827\n",
      "Recall: 0.5827\n",
      "F1: 0.5827\n"
     ]
    }
   ],
   "source": [
    "# WRITE CODE HERE\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#create a class variable using the premade code\n",
    "C = LexiconClassifier()\n",
    "\n",
    "#create an empty list to make hold the predicts\n",
    "lex_test_preds = []\n",
    "\n",
    "#loop over the test set using the predict feature of the class\n",
    "for x in X_txt_test:\n",
    "    lex_test_preds.append(C.predict(x))\n",
    "    \n",
    "#calculate model scores using y set and predicts above\n",
    "precision = precision_score(y_test, lex_test_preds, average= 'micro')\n",
    "recall = recall_score(y_test, lex_test_preds, average= 'micro')\n",
    "f1 = f1_score(y_test, lex_test_preds, average= 'micro')\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(lex_test_preds) == type(list()))\n",
    "assert(type(lex_test_preds[0]) == type(str()))\n",
    "assert(set(lex_test_preds) == set([\"positive\", \"negative\", \"neutral\"]))\n",
    "assert(len(lex_test_preds) == len(y_test))\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (20 point)\n",
    "\n",
    "Again, using the LexiconClassifier, write code to generate a lists of lists where each sublist contains the number of positive words and negative words in a tweet. Assume we are give the train test datasets\n",
    "\n",
    "``` python\n",
    "X_txt_train = [\"good good\", \"bad bad\"]\n",
    "X_txt_test = [\"great\", \"bad bad great\"]\n",
    "```\n",
    "\n",
    "you should write code that creates two lists of lists as follows:\n",
    "\n",
    "``` python\n",
    "X_train_lexicon_features = [[2, 0], [0,2]] # [2, 0] means the first tweeta has 2 positive words and 0 negative words.\n",
    "X_test_lexicon_features = [[1, 0], [1, 2]]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE\n",
    "\n",
    "#create empty lists to put lists into\n",
    "X_train_lexicon_features = []\n",
    "X_test_lexicon_features = []\n",
    "\n",
    "#create class variable\n",
    "lex = LexiconClassifier()\n",
    "\n",
    "#determine number of pos and neg words\n",
    "for x in X_txt_train:\n",
    "    X_train_lexicon_features.append([lex.count_pos_words(x),lex.count_neg_words(x)])\n",
    "\n",
    "for y in X_txt_test:\n",
    "    X_test_lexicon_features.append([lex.count_pos_words(x),lex.count_neg_words(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(X_train_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features[0]) == type(list()))\n",
    "assert(len(X_train_lexicon_features) == len(X_txt_train))\n",
    "assert(len(X_test_lexicon_features) == len(X_txt_test))\n",
    "assert(len(X_train_lexicon_features[0]) == 2)\n",
    "assert(len(X_test_lexicon_features[0]) == 2)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (30 points)\n",
    "\n",
    "For this task you should creat a feature matrix using CountVectorizer and train a LinearSVC model from scikit-learn. On the train split, use GridSearchCV to find the best LinearSVC C values (0.0001, 0.001, 0.001, 0.01, 0.1, 1, 10, or 100) based on the micro f1 scoring metric (hint: \"micro\" average) and set the cv parameter to 5. Also, with the CountVectorizer, only use unigrams (i.e., set ngram_range = (1,1)). Note that GridSearchCV will retrain the final classifier using the best parameters, so you don't need to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoffm\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.6226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "#make an empty vectorizer\n",
    "Vec = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "#fit the training and transform the test sets\n",
    "X_train = Vec.fit_transform(X_txt_train)\n",
    "X_test = Vec.transform(X_txt_test)\n",
    "\n",
    "#fit the model\n",
    "linsvc = GridSearchCV(LinearSVC(), param_grid={'C': [.0001, 100]}, \n",
    "                      scoring = 'f1_micro', cv = 5)\n",
    "linsvc.fit(X_train, y_train)\n",
    "\n",
    "#find the best score\n",
    "validation_score = linsvc.best_score_\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8018, 20864)\n",
      "(3199, 20864)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6158\n",
      "Recall: 0.6158\n",
      "F1: 0.6158\n"
     ]
    }
   ],
   "source": [
    "#make predictions using the model\n",
    "svm_test_predictions = linsvc.predict(X_test)\n",
    "\n",
    "#score the model\n",
    "precision = precision_score(y_test, svm_test_predictions, average= 'micro')\n",
    "recall = recall_score(y_test, svm_test_predictions, average= 'micro')\n",
    "f1 = f1_score(y_test, svm_test_predictions, average= 'micro')\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(type(X_train) == type(csr_matrix(0)) or type(X_train) == type(np.array(0)))\n",
    "assert(type(X_test) == type(csr_matrix(0)) or type(X_test) == type(np.array(0)))\n",
    "assert(X_train.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train.shape[1] == X_test.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (20 points)\n",
    "\n",
    "Repeat the experiment from exercise 4, but include the lexicon features with the CountVectorizer features. Specifically, you need to concatenate the variables ```X_train_lexicon_features``` and ```X_test_lexicon_features``` with ```X_train``` and ```X_test```, respectively. Intuitively, we are performing feature engineering by adding \"lexicon features\".\n",
    "\n",
    "HINT: You will need to convert the lexicon features to numpy arrays then call hstack from the scipy.sparse library (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.hstack.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8018, 20866)\n",
      "(3199, 20866)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "#turn the variables into arrays\n",
    "trainarray = np.array(X_train_lexicon_features)\n",
    "testarray = np.array(X_test_lexicon_features)\n",
    "\n",
    "#merge the two arrays horizontally\n",
    "X_train_w_lex = sp.hstack([X_train, trainarray]).toarray()\n",
    "X_test_w_lex = sp.hstack([X_test, testarray]).toarray()\n",
    "\n",
    "print(X_train_w_lex.shape)\n",
    "print(X_test_w_lex.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoffm\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\hoffm\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\hoffm\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\hoffm\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\hoffm\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\hoffm\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.6382\n",
      "Precision: 0.5649\n",
      "Recall: 0.5649\n",
      "F1: 0.5649\n"
     ]
    }
   ],
   "source": [
    "#fit the model with the new variables\n",
    "lexsvc = GridSearchCV(LinearSVC(tol = .0001), param_grid={'C': [.0001, 100]},\n",
    "                      scoring = 'f1_micro', cv = 5)\n",
    "lexsvc.fit(X_train_w_lex, y_train)\n",
    "\n",
    "validation_score = lexsvc.best_score_\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_lex_test_predictions = lexsvc.predict(X_test_w_lex)\n",
    "\n",
    "precision = precision_score(y_test, svm_lex_test_predictions, average= 'micro')\n",
    "recall = recall_score(y_test, svm_lex_test_predictions, average= 'micro')\n",
    "f1 = f1_score(y_test, svm_lex_test_predictions, average= 'micro')\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(X_train_w_lex.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train_w_lex.shape[1] == X_test.shape[1] + 2)\n",
    "assert(X_train_w_lex.shape[1] == X_test_w_lex.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (10 points)\n",
    "\n",
    "For this exercise, you will perform manual analysis of the predictions. Answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Musical awareness: Great Big Beautiful Tomorrow has an ending, Now is the time does not\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: On Radio786 100.4fm 7:10 Fri Oct 19 Labour analyst Shawn Hattingh: Cosatu's role in the context of unrest in the mining http://t.co/46pjzzl6\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Kapan sih lo ngebuktiin,jan ngomong doang Susah Susah.usaha Aja blm udh nyerah,inget.if you never try you'll never know.cowok kok gentle bgt\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: negative\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: Tomorrow come and hear @DavidWillettsMP&amp;@MASieghart debate \"Navigating the new Higher Education market\" 5.30pm, Jurys Inn #CPC12\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: Excuse the connectivity of this live stream, from Baba Amr, so many activists using only one Sat Modem. LIVE http://t.co/U283IhZ5 #Homs\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Show your LOVE for your local field &amp; it might win an award!  Gallagher Park #Bedlington current 4th in National Award http://t.co/WeiMDtQt\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: @firecore Can you tell me when an update for the Apple TV 3rd gen becomes available? The missing update holds me back from buying #appletv3\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: @Heavensbasement The Crown, Filthy McNastys, Katy Dalys or the Duke if York in Belfast! Can't wait to catch you guys tomorrow night!\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Uncover the Eternal City! Return flights to Rome travel on the 21st January, for 3 nights Augustea, 3 star Hotel... http://t.co/tw0Jeh9g\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: My #cre blog Oklahoma Per Square Foot returns to the @JournalRecord blog hub tomorrow. I will have some interesting local data to share.\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: \"@bbcburnsy: Loads from SB; talks with Chester continue; no deals 4 out of contract players 'til Jan; Dev t Roth ,Coops to Chest'ld #hcafc\"\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: negative\n",
      "SVM+Lexicon Prediction: negative\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: Trey Burke has been suspended for the Northern Michigan game (exhibition) tomorrow. http://t.co/oefkAElW\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: W.O.W Wednesday!Marni lands this Lumberjack vest for the ladies looking to bring a little Tom boy toughness  http://t.co/7NyCbdJR\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Activists in Deir Ezzor captured this image of Musab Bin Umair Mosque after regime forces set it on fire Wednesday. http://t.co/MRcoprCE\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: @karaotr You will appreciate this.. Sunday brunch coffee: Normal cup in b/g and then the BOWL of java. Yowza. http://t.co/XhbtaCvm\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: Join me Wed for a live webcast on cost optimization for IT, for the SMB crowd. http://t.co/tyJn4RES  &lt;&lt; send your questions in! #DellWebcast\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: Special THANKS to EVERYONE for coming out to Taboo Tuesday With DST tonight! It was FUN&amp;educational!!! :) @XiEtaDST\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: @fatimasule That was the revelation I mentioned on sunday evening. I am still in Abj. How are u &amp; where have u been again?\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: negative\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: Kim Hyung Jun - Football Team the 2nd A Match at YeongDeungPo-gu DaeRimDong [12.10.27] Credit : tlxhah #6 http://t.co/u7mPTl0X\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: The audio booth is ready to blow the roof off the Comcast Center tomorrow! Are you? #MDMadness http://t.co/B19fECgY\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: negative\n",
      "Lexicon Model Prediction: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_tweets = 0\n",
    "for text, svm_pred, svm_lex_pred, lex_pred, y  in zip(X_txt_test, svm_test_predictions,\n",
    "                                    svm_lex_test_predictions, lex_test_preds, y_test):\n",
    "    print(\"Tweet: {}\".format(text))\n",
    "    print(\"Ground-Truth Class: {}\".format(y))\n",
    "    print(\"SVM Prediction: {}\".format(svm_pred))\n",
    "    print(\"SVM+Lexicon Prediction: {}\".format(svm_lex_pred))\n",
    "    print(\"Lexicon Model Prediction: {}\".format(lex_pred))\n",
    "    print()\n",
    "    \n",
    "    num_tweets += 1\n",
    "    if num_tweets == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following tasks:\n",
    " \n",
    "- Manually annotate all of the tweets printed above:\n",
    "   1. Negative because it is saying the album does not have an ending\n",
    "   1. Neutral\n",
    "   1. Positive because it is saying you should try\n",
    "   1. Neutral\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "   1. Negative because it is saying the lack of update is preventing purchase\n",
    "   1. Positive\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "   1. Negative\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "   1. Negative because it is about a mosque being set on fire\n",
    "   1. Positive because it is meant to be humorous\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "   1. Neutral because it is a mix of pos and neg sentences\n",
    "   1. Neutral\n",
    "   1. Positive\n",
    "\n",
    "- How many of your annotations match the ground truth labels? Do you think the datasets labels are correct? (Use your intuition)\n",
    "    - 15. Yes, for the most part. They match my annotations enough where I would trust the training model\n",
    "\n",
    "- How many of your annotations match the lexicon-based model's predictions?\n",
    "    - 9\n",
    "\n",
    "- How many of your annotations match the SVM's predictions?\n",
    "    - 12\n",
    "    \n",
    "- How many of your annotations match the SVM+Lexicon's predictions?\n",
    "    - 11\n",
    "    \n",
    "- Do you see any major limitations of the linear SVM model? Use your intuition, I will accept most answers, as long as it makes some sense. Please describe and provide examples below:\n",
    "\n",
    "The f scores are very low, which indicates some issues with the fit. I think that a linear relationship cannot be expressed using sentiment scores because there is not a set relationship between word and sentiment. For example, tweet 1 is about an album not having an ending, but taking the words as individuals makes the prediction positive. This is because the model places a certain linear coefficient on each word without looking at the meaning of the entire tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit 1 (20 points)\n",
    "\n",
    "For this extra credit the only goal is to improve your model on the test set (i.e., increase the micro f1 score). You may create new features, grid search over more parameters, try different feature weighting methods (e.g., TfidfVectorizer), or test different machine learning models. You can do whatever you want as long as the final test score improves, I will provide you with the extra credit points.\n",
    "\n",
    "DO NOT TRAIN ON THE TEST SET. That is cheating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
