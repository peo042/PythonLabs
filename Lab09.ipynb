{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09\n",
    "### IS6713\n",
    "\n",
    "Instructions:\n",
    "1. Add the lines of code required to obtain the requested output.\n",
    "\n",
    "2. The code should be inserted where requested   \n",
    "   \n",
    "3. Do not modify other parts of this notebook.  \n",
    "  \n",
    "4. Remember to write in your name and abc123 in the first cell of this notebook.\n",
    "  \n",
    "5. Upload the notebook on Blackboard before the deadline!\n",
    "\n",
    "Name: Ben Hoffman\n",
    "\n",
    "abc123: peo042"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.1\n",
    "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").  \n",
    "The data I am providing is a reformatted version of the Kaggle one. For example, \"neutral\" Tweets are not included for simplicity. The dataset includes three fields: **airline**, **full_text**, and **gold_label**.  \n",
    "Tasks:\n",
    "- Load the **w9_airlinetwitter.jsonl** dataset\n",
    "- Vectorize using the TFIDF approach\n",
    "- Split the dataset in train (80%) and test (20%) sets\n",
    "- Train the RandomForest Classifier over the train set\n",
    "- Predict the labels of the test set using the trained model\n",
    "- Calculate and print:\n",
    "  - Classification report\n",
    "  - Confusion matrix\n",
    "  - Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: load the dataset\n",
    "## Don't modify this code\n",
    "import json\n",
    "\n",
    "dataset = []\n",
    "with open('w9_airlinetwitter.jsonl') as infile:\n",
    "    for line in infile:\n",
    "        dataset.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Correctly Loaded\n"
     ]
    }
   ],
   "source": [
    "assert(len(dataset)==11541)\n",
    "print(\"Dataset Correctly Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airline': 'Virgin America',\n",
       " 'full_text': \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       " 'gold_label': 'positive'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the format (don't modify this cell)\n",
    "## It should be something like:\n",
    "#\"\"\"{'airline': 'Virgin America',\n",
    "# 'full_text': \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
    "# 'gold_label': 'positive'}\n",
    "# \"\"\"\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginamerica plus ve added commercials experience tacky\n"
     ]
    }
   ],
   "source": [
    "## Step 2 - Vectorize the dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TV\n",
    "\n",
    "def nopun(text):\n",
    "    no_pun = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_pun\n",
    "\n",
    "goldlab, feature = [], []\n",
    "\n",
    "for d in dataset:\n",
    "    text = d['full_text'].lower()\n",
    "    token = nltk.word_tokenize(text)\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    cleantok = [t for t in token if len(t.lower())>1 and (t.lower() not in stop)]\n",
    "        \n",
    "    clean = []\n",
    "    for w in cleantok:\n",
    "        cleant = nopun(w)\n",
    "        if len(cleant.strip())>1:\n",
    "            clean.append(cleant.strip())\n",
    "            \n",
    "    goldlab.append(d['gold_label'])\n",
    "    feature.append(' '.join(clean))\n",
    "\n",
    "vector = TV(max_features = 2500, min_df = 7, max_df = 0.8, stop_words = stopwords.words('english'))\n",
    "\n",
    "processedvec = vector.fit_transform(feature).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica plus ve added commercials experience tacky'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3 - Split in train and test\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = tts(processedvec, goldlab, test_size = .2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 4: train the classifier\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "textclass = RFC(n_estimators = 200, random_state = 0)\n",
    "textclass.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Predict the labels\n",
    "pred = textclass.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 278 TN 1805 FP 55 FN 171\n",
      "[[1805   55]\n",
      " [ 171  278]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.97      0.94      1860\n",
      "    positive       0.83      0.62      0.71       449\n",
      "\n",
      "    accuracy                           0.90      2309\n",
      "   macro avg       0.87      0.79      0.83      2309\n",
      "weighted avg       0.90      0.90      0.90      2309\n",
      "\n",
      "RF Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "## Step 6: Assess the performance (Accuracy, Confusion Matrix, Classification Report)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(ytest, pred).ravel()\n",
    "\n",
    "print(\"TP\", tp, \"TN\", tn, \"FP\", fp, \"FN\", fn)\n",
    "print(confusion_matrix(ytest, pred))\n",
    "print(classification_report(ytest, pred))\n",
    "print('RF Accuracy: {0:.2f}'.format(accuracy_score(ytest, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.2 \n",
    "### IRIS\n",
    "Do the following:\n",
    "1. Load the **iris.csv** dataset into a numpy array. The first 4 columns are the  features/attributes. The last column is the class. Simply load the class as a list of strings. You can use the CVS method to load the features into a list of lists (X) and to load the gold label as a list (y). Then, convert X and y into a numpy array (hint: use np.array() for the conversion).\n",
    "2. Use train_test_split() to split X and y into train and test. (use 0.2 for the test_size)\n",
    "3. Train an SVM classifier on the train split and evaluate its accuracy on the test split. Print the accuracy.\n",
    "4. Apply PCA to the array X, reducing the number of features to 2. Save the new set of features in a different array (e.g. newX)\n",
    "4. Split the new dataset (newX) and y in train and test\n",
    "5. Train an SVM classifier on the train split and evaluate the accuracy on the test. Print the new accuracy.\n",
    "6. Compare the results from 3) and 5). Which one is better?\n",
    "  \n",
    "Note that this is a toy dataset, so all scores will be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "X = [] # Will be a list of lists\n",
    "y = [] # will be a list\n",
    "\n",
    "## Write here your code\n",
    "## read the file\n",
    "with open('iris.csv') as file:\n",
    "    for row in file:\n",
    "        column = row.strip().split(',')\n",
    "        X.append([float(x) for x in column[:-1]])\n",
    "        y.append(column[-1])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "# split into train and test\n",
    "xtrain, xtest, ytrain, ytest = tts(X,y,test_size = .2, random_state= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train an SVM Classifier, test and print the accuracy\n",
    "clf = SVC()\n",
    "clf.fit(xtrain, ytrain)\n",
    "\n",
    "pred = clf.predict(xtest)\n",
    "accuracy = accuracy_score(ytest, pred)\n",
    "print(\"The model accuracy is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Run a PCA to select only 2 components and create a new dataset\n",
    "ss = StandardScaler()\n",
    "xscale = ss.fit_transform(X)\n",
    "pca = PCA(n_components = 2)\n",
    "newx = pca.fit_transform(xscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy is 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "## Split the new dataset, train a SVM classifier, test and print the accuracy\n",
    "xtrain, xtest, ytrain, ytest = tts(newx,y,test_size = .2, random_state= 0)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(xtrain, ytrain)\n",
    "\n",
    "pred = clf.predict(xtest)\n",
    "accuracy = accuracy_score(ytest, pred)\n",
    "print(\"The model accuracy is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment about the differences. Is the difference big or small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nComment here\\n\\nIn this case, it is a big difference because it goes from 1.0 accuracy to a non-perfect (.9) accuracy.\\n\\nAlthough that is a big difference, .9 accuracy is still a very good score. Thus, does not exclude it from\\ncontention in the model building process.\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Comment here\n",
    "\n",
    "In this case, it is a big difference because it goes from 1.0 accuracy to a non-perfect (.9) accuracy.\n",
    "\n",
    "Although that is a big difference, .9 accuracy is still a very good score. Thus, does not exclude it from\n",
    "contention in the model building process.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
